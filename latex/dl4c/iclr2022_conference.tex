\documentclass[usenames,dvipsnames]{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% Packages
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{pgfplots, pgfplotstable}
\usepgfplotslibrary{fillbetween}
\usepackage[pdf]{graphviz}
\usepackage{tikz}
\usepackage{enumitem}
\usepackage{bbm}
\usepgfplotslibrary{statistics}

\usepackage{booktabs}
\usepackage{pifont}
\usepackage{fontawesome}

\newcommand{\wmark}{\textcolor{orange}{\ding{45}}}
\newcommand{\cmark}{\textcolor{green!80!black}{\ding{51}}}
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}

\usepackage{multicol}

\usepackage{bussproofs}
\usepackage{stackengine}
\usepackage{amssymb}

% Packages
\usepackage{soul}
\usepackage{listings}
\usepackage{xcolor}
\DeclareRobustCommand{\hlred}[1]{{\sethlcolor{pink}\hl{#1}}}
\usepackage{fontspec}

\setmonofont[Scale=0.8]{JetBrainsMono}[
  Contextuals={Alternate},
  Path=./JetBrainsFontFiles/,
  Extension = .ttf,
  UprightFont=*-Regular,
  BoldFont=*-Bold,
  ItalicFont=*-Italic,
  BoldItalicFont=*-BoldItalic
]

\makeatletter
\def\verbatim@nolig@list{}
\makeatother

\lstdefinelanguage{kotlin}{
  comment=[l]{//},
  commentstyle={\color{gray}\ttfamily},
  emph={delegate, filter, firstOrNull, forEach, it, lazy, mapNotNull, println, repeat, assert, with, head, tail, len, return@},
  numberstyle=\noncopyable,
  emphstyle={\color{OrangeRed}},
  identifierstyle=\color{black},
  keywords={abstract, actual, as, as?, break, by, class, companion, continue, data, do, dynamic, else, enum, expect, false, final, for, fun, get, if, import, in, infix, interface, internal, is, null, object, open, operator, override, package, private, public, return, sealed, set, super, suspend, this, throw, true, try, catch, typealias, val, var, vararg, when, where, while, tailrec, reified},
  keywordstyle={\color{NavyBlue}\bfseries},
  morecomment=[s]{/*}{*/},
  morestring=[b]",
  morestring=[s]{"""*}{*"""},
  ndkeywords={@Deprecated, @JvmField, @JvmName, @JvmOverloads, @JvmStatic, @JvmSynthetic, Array, Byte, Double, Float, Boolean, Int, Integer, Iterable, Long, Runnable, Short, String, int},
  ndkeywordstyle={\color{BurntOrange}\bfseries},
  sensitive=true,
  stringstyle={\color{ForestGreen}\ttfamily},
  literate={`}{{\char0}}1,
  escapeinside={(*@}{@*)}
}

\lstset{basicstyle=\ttfamily\lst@ifdisplaystyle\footnotesize\fi, language=kotlin}

\usepackage{url}
\usepackage{textcomp}

\title{How Robust are Neural Code Completion\\Models to Source Code Transformation?}
\author{Breandan Considine, Xiaojie Xu, Xujie Si, Jin L.C. Guo\\
\texttt{\{breandan.considine, xiaojie.xu, xsi, jguo\}@\{mail, cs\}.mcgill.ca}\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
  \maketitle

  \begin{abstract}
    Neural language models hold great promise as tools for computer-aided programming, but questions remain over their reliability and the consequences of overreliance. In the domain of natural language, prior work has revealed these models can be sensitive to naturally-occurring variance and malfunction in unpredictable ways. A more methodical examination is necessary to understand their behavior on programming-related tasks. In this work, we develop a methodology for systematically evaluating neural code completion models using common source code transformations. We measure the distributional shift induced by applying those transformations to a dataset of handwritten code fragments on four pretrained models, which exhibit varying degrees of robustness under transformation. Preliminary results from those experiments and observations from a qualitative analysis suggest that while these models are promising, they should not be relied upon uncritically. Our analysis provides insights into the strengths and weaknesses of different models, and serves as a foundation for future work towards improving the accuracy and robustness of neural code completion.
  \end{abstract}

  \section{Introduction}\label{sec:introduction}

  Neural language models (NLMs) play an increasingly synergistic role in software engineering, and are featured prominently in recent work on neural code completion~\citep{chen2021evaluating}. Code completion is the task of automatically providing a list of potential completions for missing tokens in source code. By training on a large corpus of source code, NLMs are capable of learning syntax~\citep{chirkova2020empirical, troshin2022probing}, stylistic elements like coding conventions~\citep{parr2016towards} as well as more complex patterns like documentation~\citep{hu2018deep}. The same pretrained model can be applied to a variety of programming tasks, such as code summarization and bug detection, by strategically placing the holes and conditioning the model's predictions on an appropriate sample space.

  However, there are risks associated with using NLMs~\citep{hellendoorn2017deep}. In the natural language domain, these models can be sensitive to stylistic variation, which is difficult to detect a priori and may lull users into a false sense of complacency. Likewise, applying NLMs in an unfamiliar coding context may result in predictions which are misaligned with developer intent, especially when the surrounding style, idioms, or APIs depart from the model's original training set. This phenomenon, called `domain mismatch' or `overfitting', is known to affect statistical learning writ large and the specific factors responsible for its occurrence in source code can be identified using domain-specific tests such as the ones we propose herein.

  Inspired by recent work investigating the robustness of NLMs in both natural language~\citep{wang2020cat} and source code~\citep{ramakrishnan2020semantic}, we develop a methodology for systematically evaluating neural code completion models using common source code transformations (SCTs). As a demonstration, we define a set of semantics-preserving transformations, and measure the distributional shift induced by applying them to a large collection of code fragments on four pretrained models. We compare the same models and transformations across three downstream tasks, i.e., code completion, document synthesis, and variable misuse detection. Our results provide insights into the strengths and weaknesses of those models, and serve as a foundation for future work towards improving the accuracy and robustness of neural code completion via retraining.

  \section{Related Work}\label{sec:related_work}

  Recent work in \textbf{neural language modeling} has shown impressive progress in long-range sequence prediction, starting with Vaswani et al.'s self-attention~\citep{vaswani2017attention} mechanism, to the BERT~\citep{devlin2018bert} and RoBERTa~\citep{liu2019roberta} architectures, now widely available in neural code completion models such as CodeBERT~\citep{feng2020codebert} and GraphCodeBERT~\citep{guo2021graphcodebert}. Though impressive, these models have known limitations, such as their inability to represent long-term dependencies and their sensitivity to noise in the input space~\cite{sun2020adv}. Similar studies have been undertaken to characterize the robustness of pretrained language models of source code~\citep{hellendoorn2017deep}.

  Our work is complementary to these studies in that we focus on neural language models and utilize structured program transformations as a kind of litmus test for detecting domain mismatch. We probe the models' understanding of source code using masked language modeling (MLM)~\citep{sinha2021masked}, constrained decoding to generate contextually-relevant text, and a suite of semantically-preserving SCTs to measure the shift in prediction accuracy across three downstream tasks. Each of these tasks may be viewed as a special case of \textbf{sketch-based program synthesis}~\citep{solar2008program}. Our approach also shares connections to automated software testing, self-supervised learning, and natural language inference as we describe below.

  First conceived in the \textbf{software testing} literature, metamorphic testing~\cite{chen1995metamorphic} is a concept known in machine learning as \textit{self-supervision}. When labels are scarce but invariant to certain groups of transformation or \textit{metamorphic relations}, given a finite labeled dataset, one can generate an effectively limitless quantity of synthetic data by selecting and recombining those transformations in a systematic manner. For example, computer vision models should be invariant to shift, scale and rotation: given a small dataset of labeled images, we can apply these transformations to generate much larger training or validation set. Could similar kinds of transformations exist for code? One promising avenue is to consider term-rewriting.

  Due to the precise distinction between syntax and semantics in programming languages, it is possible to generate semantically-admissible perturbations without requiring a learned similarity metric (e.g., word or sentence embedding). Using a simple \textbf{term rewriting} system with a context-sensitive grammar allows us to reason about plausible variations to source code. This idea is partly inspired by Josh Rule's work, ``The Child as a Hacker''~\citep{rule2020child}, whose thesis resonates with the authors' own experience learning to program. Coupled with a neural controller and some kind of interactive debugging mechanism, we believe this approach offers a compelling alternative to imitation learning (à la supervision on a gigantic corpus of text), and more faithfully resembles the way humans learn to write code.

  Similar research has been undertaken~\citep{weiss2018practical, merrill2021power} to characterize the \textbf{formal grammars} which neural language models can recognize in theory. Our work builds on this literature from a practical standpoint: we investigate how neural code completion models respond to plausible cosmetic variation in real-world code fragments. Though rudimentary, our work can be seen as an early attempt to study how neural language models behave in realistic programming scenarios and lays the foundation for future work towards bridging the gap between natural and programming language understanding. By partitioning the Chomsky hierarchy into fine-grained semantic categories, we hope to precisely characterize the expressive power of neural code completion models via their understanding of programming language concepts.

  Towards this end, we incorporate several ideas from \textbf{programming language theory} to probe the models' understanding of source code. For example, we leverage the concept of contextual equivalence~\citep{morris1969lambda} to justify the validity of our SCTs. We borrow the concept of capture-avoiding substitution from $\lambda$-calculus to prevent name collision under variable renaming. We adapt ideas from flow analysis to reorder statements which share no dataflow dependencies. Taken together, these ideas provide a principled way to reason about the correctness of our transformations and their impact on the underlying program's semantics. We believe that ideas from PL theory have been largely overlooked by the ML for code community and represent a significant opportunity for future research on code completion.

  Last but not least, we tap into work on natural language inference and question-answering~\citep{liu2021codeqa}. This work contains a rich set of ideas for testing program comprehension in neural language models and may be useful for bug detection and automatic program repair.

  \pagebreak\section{Background}\label{sec:background}

  The same program can be written in many possible ways. Often, those variants contain superficial changes to the source code, but do not meaningfully alter the behavior of the underlying program. Regardless of the surrounding context, neural language models should not become suddenly confused when presented with a cosmetically altered program: a model trained on a language semantically invariant to certain rewrites should itself exhibit invariance under those same rewrites. However this property may not necessarily hold.

  As documented by prior literature, neural language models can be prone to semantic drift that is not supported by the underlying language semantics. The question becomes, how do we estimate the robustness of those models to rewrites which are known \textit{a priori} to be cosmetic in nature? Our work addresses this question by identifying four classes of cosmetic rewrites, applies them to a dataset of Java source code, and measures the relative drift of four SoTA pretrained language models across three separate code completion tasks.

We have identified three high-level categories of source code transformations:

  \begin{enumerate}[itemsep=1ex]
    \item \textbf{Syntactic}, which may produce a valid or invalid parse tree. For example: simple refactoring, typos, imbalanced parentheses, unparsable code.
    \item \textbf{Semantic}, which may either preserve or alter the program's meaning. For example: functional code clones, dis-equal constant or expression rewriting.
    \item \textbf{Cosmetic}, which is strictly superficial. For example: variable renaming, independent statement reordering, extra documentation, dead code, or logging.
  \end{enumerate}

  In contrast with syntactic or semantic transformations, cosmetic transformations are semantically identical, syntactically valid and only superficially alter syntactic structure. We show that even in this highly restrictive space of transformations, source code has many degrees of freedom: two authors implementing the same function may select different variable names or other cosmetic features, such as whitespaces, diagnostic statements or comments.  Yet our results suggest that even in this narrow space of transformations, SoTA neural code completion models can be surprisingly sensitive to noise in the cosmetic domain.

  \section{Method}\label{sec:method}

  Our goal is to measure the robustness of SoTA neural code completion models on natural code fragments exposed to various cosmetic transformations. Hence, we construct one SCT from each of the following four categories of cosmetic changes:

  \begin{enumerate}[itemsep=1ex]
    \item \textbf{Synonym renaming}: substitutes variable names with synonyms.
    \item \textbf{Extraneous code}: sprinkles non-essential statements into code.
    \item \textbf{Statement reordering}: reorders dataflow independent statements.
    \item \textbf{Permuted argument order}: scrambles user-defined method arguments.
  \end{enumerate}

  Put simply, we use the following rewriting criteria to produce semantically admissible SCTs:

  \begin{enumerate}[itemsep=1ex]
    \item The \textsc{RenameTokens} SCT substitutes each CamelCase subword in the most frequent user-defined token with a uniformly-sampled lemma from its WordNet hypernym ego graph up to three hops away, representing an alternately-chosen (e.g., variable or function) name of similar meaning.
    \item The \textsc{AddExtraLogging} SCT adds intermittent print statements in linear chains of code, with a single argument synthesized by the code completion model for added variation. More generally, this can be any superfluous statement which does not change the runtime semantics.
    \item The \textsc{SwapIndepLines} SCT swaps adjacent lines of equal scope and indentation which share no tokens in common. Although it may introduce semantic drift in some code fragments, this SCT ideally represents an alternate topsort on the DFG.
    \item The \textsc{PermuteArguments} SCT shuffles the arguments of a user-defined function of dyadic or higher-arity, representing an alternate parameter order of a project-local function (i.e., defined outside the standard library).
  \end{enumerate}

  For a more technical description of our rewrite semantics, please refer to Appendix~\ref{sec:rewrite_semantics}.

  Three tasks are considered. Each task uses a custom mask generator, allowing us to simulate multiple downstream tasks with the same test harness, only modifying the mask locations and admissible completions. Each task is scored with a task-appropriate metric. For single-token completion, we compare the top-1 precision across the full vocabulary and all masks:

  \begin{equation*}
    \:\text{Precision@1} = \frac{1}{{|\textsc{Tokens}|}}\sum_{\texttt{tok}\in \textsc{Tokens}}\mathbbm{1}\:\{\textsc{Model}(\textsc{Code}\:\{\:\small{\texttt{tok}}\:\mapsto\small{\texttt{<mask>}}\:\}) \overset{?}{=} \:\textsc{Code}\}
  \end{equation*}

  For document synthesis, we use a variant of the ROUGE score measuring synonym overlap:

  \begin{equation*}
    \text{ROUGE-synonym} = \Delta_{\text{Syn}}(\textsc{Model}^{k}(\textsc{Code}\:\{\:\small{\texttt{//comment}}\:\mapsto\texttt{//<mask>}\:\}), \:\texttt{//comment})
  \end{equation*}

  where $\textsc{Model}^{k}$ denotes autoregressively sampling $k$ consecutive tokens, and $\Delta_\text{Syn}$ denotes the unigram synonym overlap between the original and synthetic comments.

  For question answering, i.e., variable misuse, we use mean reciprocal rank (MRR) with a set of multiple-choice distractors, \textsc{Dupes}, sampled uniformly from the surrounding context:

  \begin{equation*}
    \text{MRR} = \frac{1}{|\textsc{Tokens}|} \sum_{\texttt{tok}\in\textsc{Tokens}} \textsc{Rank}\big(\texttt{tok}, \textsc{Model}(\textsc{Code}\:\{\:\texttt{tok}\:\mapsto\texttt{<mask>}\:\}, \texttt{tok}\cup\textsc{Dupes})\big)^{-1}
  \end{equation*}

  For code completion, we uniformly sample and mask $N$ individual tokens in both the original and transformed code fragment for evaluation. We then collect the model's highest-scoring predictions for each mask location, and average the completion accuracy on the original and transformed code fragment. An example of one such scenario is provided in Fig.~\ref{fig:code_completion}.

  Similarly, for document synthesis, we mask a naturally-occurring comment and autoregressively synthesize a new one in its place, then compare the ROUGE-scores of the synthetic documents before and after transformation. An example of may be found in Fig.~\ref{fig:document_synthesis}.

  Our dataset was constructed by cloning a hundred of the highest-starring Java repositories hosted by GitHub organizations with over 100 forks and between 1 and 10 MB in size, sorted by issue activity. Selecting projects matching these criteria will retrieve a diverse collection of active repositories with enterprise-level code quality and reasonable stylistic diversity.

  We conducted a full-factorial randomized trial consisting of four state-of-the-art pretrained models (GraphCodeBERT, CodeBERT, CodeBERT-Small, RoBERTa-Java), four SCTs, and three downstream tasks (code completion and document synthesis). While the total samples per-model and per-task may vary, we provide the same wall clock time (360 minutes) and hardware resources (nVIDIA Tesla V100) to each model. The number of code fragments that can be evaluated in the allotted time varies depending on the architecture and task, but in each case, the significant figures have mostly converged.

  Figure~\ref{fig:semantic_drift} displays the results of this process. Each table represents a 2D slice of the hypercube corresponding to a single task, with dual box-and-whisker plots for each model, before and after applying the transformation listed on the horizontal-axis. The raw data cube generated by our experimental pipeline can be found in Appendix~\ref{sec:detailed_results}.

  \section{Results and discussion}\label{sec:results}

  \vspace{-10pt}\begin{figure}[H]
          \centering\hspace*{-0.6cm}
          \includegraphics[width=0.38\textwidth]{figs/Code Completion}\hspace*{-0.6cm}
          \includegraphics[width=0.38\textwidth]{figs/Document Synthesis}\hspace*{-0.6cm}
          \includegraphics[width=0.38\textwidth]{figs/Variable Misuse}
          \caption{Semantic shift across all tasks (code completion, document synthesis and variable misuse), models (GraphCodeBERT, CodeBERT, CodeBERT-Small, RoBERTa-Java) and SCTs (\textsc{Rename}, \textsc{AddExtraLogging}, \textsc{SwapIndepLines}, \textsc{PermuteArguments}).}
      \label{fig:semantic_drift}
  \end{figure}

  A careful examinination of the results in Fig.~\ref{fig:semantic_drift} allows us to draw the following conclusions:

  \begin{itemize}
    \item Models specifically intended to handle source code, (i.e., CodeBERT and GraphCodeBERT) are more robust than those designed for text alone (i.e., RoBERTa).
    \item RoBERTa would appear to be uniquely sensitive to \textsc{PermuteArguments} and \textsc{AddExtraLogging}, a pathology which none of its code cousins shared.
    \item \textsc{RenameTokens} appears to have a detrimental effect across the board.
    \item \textsc{SwapIndepLines} seems to have a negligible effect. This result aligns with the word order findings reported by Sinha et al.~\citep{sinha2021masked} in natural language.
    \item Our results lend credence to the relative model rankings reported by prior literature: RoBERTa $\ll$ CodeBERT $<$ GraphCodeBERT.
    \item Document synthesis has a fairly high variance. This may be due to poor alignment between the metric and the task. While comments are qualitatively passable, the $\Delta_{\text{Sym}}$ metric does poorly at discriminating between good comments and bad ones.
  \end{itemize}

 For samples from the code and document synthesizer, we refer the reader to Appendix~\ref{sec:examples}.

  \section{Future work}

  % For simplicity's sake, we implemented our transformations using a set of ad-hoc regular expressions, however this approach has some shortcomings. Although RegExs are easy to implement and do support rudimentary transformations, they are a crude way to manipulate source code. In order to generate semantically valid transformations, one should really use a proper metaprogramming library. Several were evaluated, including OpenRewrite, TXL~\citep{cordy2004txl}, Refal~\citep{gurin1991refal} et al., but their features were ultimately found wanting (e.g., poor error recovery) and the complexity of using them (e.g., parsing, API integration) proved too burdensome.

  Our SCTs can be viewed as ``possible worlds'' in the tradition of modal logic: the original author plausibly could have written the same procedure in a slightly different form. Although we are unable to access all these worlds, we can posit the existence and likelihood of some, and given a dataset of code fragments, begin to probe a candidate model's predictions.

  One intriguing avenue for future work would be to consider combinations of source code transformations. This would vastly expand the cardinality of the validation set, enabling us to access a much larger space of possible worlds, albeit potentially at the risk of lower semantic admissibility, as arbitrary combinations of SCTs can quickly produce invalid code. This presents an interesting engineering challenge and possible extension to this work.

  Although we currently only use average precision, ROUGE-synonym and mean reciprocal rank, it would be useful to report Kantorovich-Rubinstein distance and other probability metrics. In addition to their utility for evaluating model robustness, these metrics can also be used to retrain those same models, a direction we hope to explore in future work.

  Finally, one could imagine using the code completion model itself to generate code for testing the same model. We have implemented this functionality to a limited extent in the \textsc{AddExtraLogging} SCT, in which the model synthesizes a single token to log, and the \textsc{InsertComment} SCT, where the model inserts a short comment. While this approach could be a useful way to generate additional training data, it would would require careful monitoring and postprocessing to avoid introducing unintended feedback loops.

  \section{Conclusion}\label{sec:conclusion}

  Neural language models hold much promise for improved code completion, however complacency can lead to increased reviewer burden or more serious technical debt if widely adopted. While trade secrecy may prevent third-party inspection of pretrained models, users would still like some assurance of their model's robustness to naturally-occurring variance. Our work helps to address this use case by treating the model as a black box: it does not require direct access to the model parameters or training data to evaluate its generalization ability.

  Our contributions in this work are twofold: we demonstrate that SoTA neural language models for source code, despite their effectiveness on long-range sequence prediction tasks, are unpredictable in the presence of specifically-constructed cosmetic variation. We also describe a systematic approach and open source implementation of a newly-developed software toolkit which allows users to empirically probe a candidate model's robustness to various categories of syntactic and semantic source code transformations. Our results are fully reproducible and can be found at: \url{https://anonymous.4open.science/r/cstk-dl4c}.

  We have shown that despite their impressive performance on certain tasks, neural code completion models are still susceptible to domain mismatch and can be sensitive to rewriting. This can be identified by designing specific rewriting scenarios, such as the ones we propose, that target the specific factors responsible for overfitting in source code. Our work can help developers better understand the strengths and weaknesses of different models and empirically compare the accuracy and robustness of neural code completion systems.

  \bibliography{iclr2022_conference}
  \bibliographystyle{plain}
  \pagebreak
  \appendix
  \section{Rewrite semantics}\label{sec:rewrite_semantics}

  We define the following four transformations to illustrate the idea of semantic invariance.

  \begin{center}
    \begin{prooftree}
      \AxiomC{Γ \vdash \:\texttt{function(args)}}
      \AxiomC{$\texttt{function} \not\in \textsc{StandardLibrary}$}
      \RightLabel{\textsc{PermuteArguments}}
      \BinaryInfC{$\Gamma \vdash \textsc{Code}\:\{\texttt{function(args)} \mapsto\texttt{function(}\faRandom\texttt{args)}\}$}
    \end{prooftree}

    \begin{prooftree}
      \AxiomC{Γ \vdash \,\,\stackanchor{\textvisiblespace\texttt{line\_1;}}{\textvisiblespace\texttt{line\_2;}}}
      \AxiomC{\textsc{Tokens}(\texttt{line\_1}) \cap\:\textsc{Tokens}(\texttt{line\_2}) = \varnothing}
      \RightLabel{\textsc{SwapIndepLines}\footnotemark{}}
      \BinaryInfC{Γ \vdash \,\,\stackanchor{\textvisiblespace\texttt{line\_2;}}{\textvisiblespace\texttt{line\_1;}}}
    \end{prooftree}

    \begin{prooftree}
      \AxiomC{Γ \vdash \:\texttt{var} \in\:\textsc{ScopedNames}}
      \AxiomC{\stackanchor{\textvisiblespace\texttt{line\_1;}}{\textvisiblespace\texttt{line\_2;}}}
%        \AxiomC{\texttt{print} \not\in\:\textsc{Tokens}(\texttt{line\_1} \:\oplus\, \texttt{line\_2})}
      \RightLabel{\textsc{AddExtraLogging}\footnotemark[\value{footnote}]}
      \BinaryInfC{\Shortstack[l]{{\vphantom{\texttt{line\_1;}}} {Γ \vdash} {\vphantom{\texttt{line\_1;}}}}  \:\Shortstack[l]{{\textvisiblespace\texttt{line\_1;}} {\textvisiblespace\texttt{println(var);}} {\textvisiblespace\texttt{line\_3;}}}}
    \end{prooftree}

    \begin{prooftree}
      \AxiomC{Γ \vdash \:\texttt{var} \in\:\textsc{ScopedNames}}
      \AxiomC{\texttt{fresh} \in\:\textsc{Synonyms}(\texttt{var}) \setminus\:\textsc{ScopedNames}}
      \RightLabel{\textsc{Rename}}
      \BinaryInfC{$\Gamma \vdash \textsc{Code}\:\{\texttt{var} \mapsto\texttt{fresh}\}$}
    \end{prooftree}
  \end{center}
  \footnotetext{The $\textvisiblespace$ symbol denotes contiguous lines of source code with the same whitespace indentation.}

  For example, let $\textsc{Code} := \lstinline|int old = 0; while (<mask> < 10) {...}|$ and assume we have a pretrained language model $\mathcal{M}_\theta: \lstinline|String|\rightarrow\lstinline|String|$ which gives the completion:

  \begin{equation}
    \mathcal{M}_\theta[\textsc{Code}] \vdash \textsc{Code}\:\{\texttt{<mask>} \mapsto \:\texttt{old}\}
  \end{equation}

  If we then rename the variable \texttt{old} to \texttt{fresh} inside the fragment \textsc{Code}, we would expect our language model's completion to shift in kind, a property we call \textit{name equivariance}. Importantly, we require fresh name generation to ensure capture-avoiding substitution:

  \begin{center}
    \begin{prooftree}
      \AxiomC{Γ \vdash \:$\mathcal{M}_{\theta}$}
      \AxiomC{\texttt{code}: \textsc{Code}}
      \AxiomC{$\{\texttt{old} \mapsto\texttt{fresh}\}: \textsc{Name} \rightarrow \:!\textsc{Name}$}
      \RightLabel{\textsc{NameEquivariance}}
      \TrinaryInfC{$\Gamma \vdash \mathcal{M}_{\theta}[\texttt{code}\:\{\texttt{old} \mapsto\texttt{fresh}\}] = \mathcal{M}_{\theta}[\texttt{code}]\{\texttt{old} \mapsto\texttt{fresh}\}$}
    \end{prooftree}
  \end{center}

  Consider another: if we reorder the arguments of a project-local function, we would expect the model to perform identically over all masked tokens, said arguments notwithstanding:

  \begin{center}
    \begin{prooftree}
      \AxiomC{Γ \vdash \:$\mathcal{M}_{\theta}$}
      \AxiomC{\texttt{code}: \textsc{Code}}
      \AxiomC{\texttt{fn(args)}: \textsc{ProjectLocal}}
      \RightLabel{\textsc{ArgOrderInvariance}}
      \TrinaryInfC{$\Gamma \vdash \mathcal{M}_{\theta}[\texttt{code}] = \mathcal{M}_{\theta}[\texttt{code}\:\{\texttt{fn(...)} \mapsto\texttt{fn(}$\faRandom$\texttt{args)}\}]$}
    \end{prooftree}
  \end{center}

  \pagebreak\section{Detailed Results}\label{sec:detailed_results}

  We evaluated three tasks: code completion, documentation synthesis and variable misuse.

  \begin{center}
    \begin{table}[H]
      \resizebox{\textwidth}{!}{%
        \begin{tabular}{r|cccc}
          Pretrained Language Model  ($\mu$, $\sigma$)                       & \textsc{Rename}                  & \textsc{PermuteArgument}               & \textsc{SwapIndepLines}               & \textsc{AddExtraLogging}               \\\hline\\
          microsoft/codebert-base-mlm \stackanchor{Before}{After}   & \stackanchor{(0.7224, 0.2768)}{(0.6100, 0.3085)} & \stackanchor{(0.7194, 0.2191)}{(0.6978, 0.2227)} & \stackanchor{(0.7089, 0.2624)}{(0.7058, 0.2607)} & \stackanchor{(0.6996, 0.2077)}{(0.7122, 0.2001)}\\\\
          microsoft/graphcodebert-base \stackanchor{Before}{After}   & \stackanchor{(0.7230, 0.2767)}{(0.6410, 0.2998)} & \stackanchor{(0.7227, 0.2173)}{(0.7154, 0.2169)} & \stackanchor{(0.7088, 0.2624)}{(0.7058, 0.2607)} & \stackanchor{(0.7050, 0.2047)}{(0.7382, 0.1932)}\\\\
          dbernsohn/roberta-java \stackanchor{Before}{After}   & \stackanchor{(0.7219, 0.2773)}{(0.4951, 0.3165)} & \stackanchor{(0.7196, 0.2188)}{(0.5995, 0.2632)} & \stackanchor{(0.7087, 0.2624)}{(0.7058, 0.2607)} & \stackanchor{(0.6958, 0.2095)}{(0.5479, 0.2577)}\\\\
          huggingface/CodeBERTa-small-v1 \stackanchor{Before}{After}   & \stackanchor{(0.7226, 0.2771)}{(0.5953, 0.3134)} & \stackanchor{(0.7216, 0.2149)}{(0.7096, 0.2249)} & \stackanchor{(0.7088, 0.2624)}{(0.7058, 0.2607)} & \stackanchor{(0.7004, 0.2095)}{(0.7113, 0.2119)}\\\\
        \end{tabular}
      }
      \caption{\label{tab:code-comp}Mean and standard deviation for single-token code completion accuracy.}
    \end{table}
    \begin{table}[H]
      \resizebox{\textwidth}{!}{%
        \begin{tabular}{r|cccc}
          Pretrained Language Model  ($\mu$, $\sigma$)                       & \textsc{Rename}                  & \textsc{PermuteArgument}               & \textsc{SwapIndepLines}               & \textsc{AddExtraLogging}               \\\hline\\
          microsoft/codebert-base-mlm \stackanchor{Before}{After}   & \stackanchor{(0.0385, 0.2684)}{(0.0430, 0.2996)} & \stackanchor{(0.0427, 0.3500)}{(0.0411, 0.3091)} & \stackanchor{(0.0309, 0.2477)}{(0.0419, 0.2959)} & \stackanchor{(0.0545, 0.4637)}{(0.0510, 0.3839)}\\\\

          microsoft/graphcodebert-base \stackanchor{Before}{After}   & \stackanchor{(0.0987, 0.3826)}{(0.1026, 0.3590)} & \stackanchor{(0.0983, 0.3214)}{(0.0982, 0.3212)} & \stackanchor{(0.0891, 0.4181)}{(0.0989, 0.3549)} & \stackanchor{(0.0850, 0.3000)}{(0.0974, 0.2221)}\\\\

          dbernsohn/roberta-java \stackanchor{Before}{After}   & \stackanchor{(0.0827, 0.5086)}{(0.0891, 0.5484)} & \stackanchor{(0.0727, 0.4453)}{(0.0781, 0.4792)} & \stackanchor{(0.0793, 0.4769)}{(0.0845, 0.5428)} & \stackanchor{(0.0680, 0.5349)}{(0.0733, 0.4994)}\\\\

          huggingface/CodeBERTa-small-v1 \stackanchor{Before}{After}   & \stackanchor{(0.1101, 0.3932)}{(0.1143, 0.3645)} & \stackanchor{(0.1205, 0.4148)}{(0.1170, 0.3668)} & \stackanchor{(0.0997, 0.3912)}{(0.1125, 0.3811)} & \stackanchor{(0.1252, 0.5175)}{(0.1230, 0.3891)}\\\\
        \end{tabular}
      }
      \caption{\label{tab:doc-synth}Mean and standard deviation ROUGE-synonym score for document synthesis.}
    \end{table}
    \begin{table}[H]
      \resizebox{\textwidth}{!}{%
        \begin{tabular}{r|cccc}
          Pretrained Language Model  ($\mu$, $\sigma$)                       & \textsc{Rename}                  & \textsc{PermuteArgument}               & \textsc{SwapIndepLines}               & \textsc{AddExtraLogging}               \\\hline\\
          microsoft/codebert-base-mlm \stackanchor{Before}{After}   & \stackanchor{(0.8150, 0.2502)}{(0.7508, 0.2663)} & \stackanchor{(0.7692, 0.2947)}{(0.7574, 0.2992)} & \stackanchor{(0.7935, 0.2683)}{(0.7861, 0.2720)} & \stackanchor{(0.7568, 0.3086)}{(0.7412, 0.3297)}\\\\

          microsoft/graphcodebert-base \stackanchor{Before}{After}   & \stackanchor{(0.8196, 0.2478)}{(0.7638, 0.2638)} & \stackanchor{(0.7758, 0.2873)}{(0.7673, 0.2919)} & \stackanchor{(0.7952, 0.2670)}{(0.7861, 0.2720)} & \stackanchor{(0.7529, 0.3071)}{(0.7378, 0.3376)}\\\\

          dbernsohn/roberta-java \stackanchor{Before}{After}   & \stackanchor{(0.8149, 0.2486)}{(0.6708, 0.2282)} & \stackanchor{(0.7727, 0.2947)}{(0.7148, 0.2399)} & \stackanchor{(0.7952, 0.2670)}{(0.7861, 0.2720)} & \stackanchor{(0.7620, 0.3053)}{(0.6797, 0.2580)}\\\\

          huggingface/CodeBERTa-small-v1 \stackanchor{Before}{After}   & \stackanchor{(0.8182, 0.2480)}{(0.7569, 0.2334)} & \stackanchor{(0.7736, 0.2934)}{(0.8023, 0.2318)} & \stackanchor{(0.7954, 0.2670)}{(0.7861, 0.2720)} & \stackanchor{(0.7578, 0.3053)}{(0.8137, 0.2413)}\\\\
        \end{tabular}
      }
      \caption{\label{tab:var-misuse}Mean and standard deviation MRR for the variable misuse detection task.}
    \end{table}
  \end{center}

  One can view each of the above tables as 2D projections of a hyperdistribution across the Cartesian product of all independent variables. To improve visual clarity and distinguish its most salient features, we apply a manual dimensionality reduction to reproject the datacube into boxplot-space, which is then colorfully rendered in Fig.~\ref{fig:semantic_drift} for the reader's convenience.

  \section{Experimental architecture}

  Though primarily an empirical study, this work also showcases a software framework for evaluating neural code completion models. It offers a number of advantages from an engineering standpoint: due to its functional implementation, it is efficient, parallelizable and highly modular, allowing others to easily reuse and extend our work with new benchmarks.

  The design of this framework is to our knowledge, unique, and merits some discussion. The entire pipeline from data mining to preprocessing, evaluation and table generation is implemented as a pure functional program in the point-free style. Given a code completion model \lstinline|cc: String->String|, a list of code fragments, \lstinline|snps: List<String>|, a masking procedure, \lstinline|msk: String->String|, an SCT, \lstinline|sct: String->String|, and a single metric over code fragments, \lstinline|mtr: (String, String)->Float|, we measure the average relative discrepancy before and after applying \lstinline|sct| to \lstinline|snps|:

  \noindent\begin{lstlisting}[basicstyle=\footnotesize\ttfamily, language=kotlin,label={lst:lstlisting}]
  fun evaluate(cc, snps, msk, sct, mtr) = Δ(
    zip(snps, snps | msk | cc) | mtr | average,
    zip(snps | sct, snps | sct | msk | cc) | mtr | average
  )
  \end{lstlisting}

  \noindent where \texttt{|} maps a function over a sequence, and \lstinline|zip| zips two sequences into a sequence of pairs. We assume \lstinline|snps| and \lstinline|msk| are fixed, and evaluate three neural code completion models across four different SCTs, and three separate tasks.

  Using our framework, it is possible to view the marginals of a rank-n tensor, representing an n-dimensional hyper-distribution formed by the Cartesian product of all variables under investigation (e.g., SCT, task, model). During evaluation, we sample these independent variables uniformly using a quasirandom sequence to ensure entries are evenly populated. We then record the first and second moments of the dependent variable of interest using a sketch-based histogram. Results are continuously delivered to the user, who may preview 2D marginals of any pair and watch the error bounds grow tighter as additional samples are drawn. This feature is indispensable when running on preemptible infrastructure and can be trivially parallelized to increase the experiment's statistical power or sweep through more expansive regions of hyperparameter or experimental design space.

  \section{Examples}\label{sec:examples}

  \begin{figure}[H]
    \begin{center}
      \begin{tabular}{|p{5cm}|p{5cm}|}
        \hline\\[-1em]1.a) Original method  &  1.b) Synonymous variant\\[-1em]\\\hline
        \begin{lstlisting}
  public void flush(int b) {
    buffer.write((byte) b);
    buffer.compact();
  }
        \end{lstlisting} & \begin{lstlisting}
  public void flush(int b) {
    (*@\hlred{cushion}@*).write((byte) b);
    (*@\hlred{cushion}@*).compact();
  }
        \end{lstlisting}
        \\\hline\\[-1em]2.a) Multi-masked method   &  2.b) Multi-masked variant\\[-1em]\\\hline
        \begin{lstlisting}
  public void <MASK>(int b) {
    buffer.<MASK>((byte) b);
    <MASK>.compact();
  }
        \end{lstlisting} & \begin{lstlisting}
  public void <MASK>(int b) {
    cushion.<MASK>((byte) b);
    <MASK>.compact();
  }
        \end{lstlisting}
        \\\hline\\[-1em]3.a) Model predictions  &  3.b) Model predictions\\[-1em]\\\hline
        \begin{lstlisting}
  public void (*@\hl{output}@*)(int b) {
    buffer.write((byte) b);
    buffer.compact();
  }
        \end{lstlisting} & \begin{lstlisting}
  public void (*@\hl{append}@*)(int b) {
    cushion.(*@\hl{add}@*)((byte) b);
    cushion.compact();
  }
        \end{lstlisting} \\\hline
      \end{tabular}
    \end{center}
    \caption{Here, we apply the \textsc{RenameTokens} SCT, then mask various tokens in the surrounding context and report the model's predictions. In this example, the model correctly predicts $\frac{2}{3}$ masks in the original method and $\frac{1}{3}$ after renaming.}
    \label{fig:code_completion}
  \end{figure}

  \begin{figure}[H]
    \begin{center}
      \begin{tabular}{|p{9cm}|}
        \hline\\[-1em]1.) Original method with ground truth document \\[-1em]\\\hline
        \begin{lstlisting}
  public void testBuildSucceeds(String gradleVersion) {
    setup(gradleVersion);
    // Make sure the test build setup actually compiles
    BuildResult buildResult = getRunner().build();
    assertCompileOutcome(buildResult, SUCCESS);
  }
        \end{lstlisting}
        \\\hline\\[-1em]2.) Synthetic document before applying SCT \\[-1em]\\\hline
        \begin{lstlisting}
  public void testBuildSucceeds(String gradleVersion) {
    setup(gradleVersion);
    // (*@\hl{build the tests with gradletuce compiler}@*)
    BuildResult buildResult = getRunner().build();
    assertCompileOutcome(buildResult, SUCCESS);
  }
        \end{lstlisting}
        \\\hline\\[-1em]3.) Synthetic document after applying SCT \\[-1em]\\\hline
        \begin{lstlisting}
  public void testBuildSucceeds(String (*@\hlred{gradleAdaptation}@*)) {
    setup((*@\hlred{gradleAdaptation}@*));
    // (*@\hl{build the actual code for test suite generation}@*)
    BuildResult buildResult = getRunner().build();
    assertCompileOutcome(buildResult, SUCCESS);
  }
        \end{lstlisting}\\\hline
      \end{tabular}
    \end{center}
    \caption{Here, we apply the \textsc{Rename} SCT, then mask the comment on line 3 and autoregressively sample tokens from the decoder to generate two synthetic comments, one before and one after applying the SCT. The stemmed synonym overlap is \texttt{build, compile, test, } before applying the SCT and \texttt{actual, build, test} afterwards.}
    \label{fig:document_synthesis}
  \end{figure}

  Initially, we seeded the document completion using \lstinline|//<MASK>| and applied a greedy autoregressive decoding strategy, recursively sampling the softmax top-1 token and subsequently discarding all malformed comments. This strategy turns out to have a very high rejection rate, due to its tendency to produce whitespace or unnatural language tokens (e.g., greedy decoding can lead to sequences like \lstinline|// ///// //| or temporarily disabled code, e.g., \lstinline|// System.out.println("debug")|). A simple fix is to select the highest-scoring prediction with natural language characters. By conditioning the decoder on at least one alphabetic character per token, one obtains more coherent documentation and rejects fewer samples from the resulting comment. It is possible to construct a more sophisticated natural language filter, however the authors did not explore this idea in great depth.

\section{Tokenization}

  Let $\Sigma$ be any alphabet (in practice, CodeBERT uses UTF-8 although it is possible to encode Chinese by switching to UTF-16).
  Let $\texttt{dict} \subset \Sigma^*\leftrightarrow\mathbb Z$ denote a bijection between certain strings over $\Sigma$ and the integers.
  Let $\texttt{bpe}: \Sigma^*\rightarrow\mathbb{Z}^*$ be an encoder that maps strings $\Sigma^*$ to a list of integers $\mathbb{Z}^*$.
  \texttt{bpe} is defined as follows: $\texttt{bpe(s)} := \texttt{dict(s[1:p])}\:\oplus\:\texttt{bpe(s[p+1:|s|])}$ where $p = \max\:\{ i \in \texttt{(1,|p|]} | \texttt{s[1:i]} \in \texttt{dict} \}$, $\oplus$ denotes list concatenation, and $\texttt{s[a:b]}$ denotes the substring of \texttt{s} between indices a and b using 1-based indexing.
  \texttt{dict} has the following property: $\forall \texttt{s} \in \Sigma^*, \texttt{bpe(s)} = [i_1, i_2, \ldots, i_n]$ implies $\texttt{dict}^{-1}(i_1) \oplus \texttt{dict}^{-1}(i_2) \oplus \texttt{dict}^{-1}(\ldots) \oplus \texttt{dict}^{-1}(i_n) = \texttt{s}$, i.e., \texttt{bpe} is a lossless compression scheme.
  Furthermore, \texttt{dict} typically has the property that $\mathbb{E}[|\texttt{bpe(s)}|] \ll \mathbb{E}[|\texttt{s}|]$ over $\texttt{s} \in L \subset \Sigma^*$ where L is a language in $\Sigma^*$ (e.g., NL or PL) -- n.b., this is untrue if $L = \Sigma^*$ due to the pigeonhole principle.

\end{document}
