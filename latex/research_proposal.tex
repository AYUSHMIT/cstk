%! Author = breandan
%! Date = 2/22/21

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{pgfplots}
\usepackage{filecontents}

% Document

\title{Learning to navigate, read and apply\\software documentation like a human}
\author{Breandan Considine, Xujie Si, Jin Guo}
\begin{document}
\maketitle

\section{Introduction}

Humans are adept information foraging agents. We can quickly find relevant information in a large corpus by recognizing and following textual landmarks. Software projects are composed of a variety of semi-structured documents containing many clues where relevant information may be found. In this work, we train a model to navigate and read software artifacts like source code and documentation, in order to facilitate common programming tasks such as code completion or defect prediction.

%Our work broadly falls under the umbrella of text-based reinforcement learning. Prior literature falls under two categories: natural or formal language. Reinforcement learning (RL) in the natural domain typically focuses on question answering~\cite{buck2017ask, chen2019reinforcement}, or interactive text games~\cite{he2015deep,ammanabrolu2018playing,narasimhan2015language,guo2020interactive,ammanabrolu2020graph}. RL techniques have begun to show promising results for program synthesis~\cite{ellis2019write, johnson2020learning, chen2020program}. Our work falls at the intersection of these two domains.

Early work in program learning realized the importance of graph-based representations~\cite{allamanis2017learning}, however explicit graph construction requires extensive feature-engineering. More recent work in program synthesis has explored incorporating a terminal~\cite{ellis2019write}, graphical~\cite{walke2020learning} or other user interface to explore the space of valid programs, however do not consider the scope or variety of artifacts in a software project. Others have shown the feasibility of learning a local graph~\cite{johnson2020learning} from source code, but still require an explicit parser to form the initial graph and adapting to settings where style, terminology and document structure vary remains a challenge.

%Unlike prior work, we consider the whole project and related artifacts. Instead of parsing their contents explicitly, which may be computationally expensive or too large to fit in memory, we allow the agent to construct the graph organically by exploring the filesystem, which can later be used to predict a label or sequence at inference time, depending on the task.

Imagine a newly-hired developer, who has programming experience, but no prior knowledge about a closed-source project. She receives access to the team's Git repository and is assigned her first ticket: Fix test broken by \texttt{0fb98be}. After locating the commit and becoming familiar with the code, she queries StackOverflow, discovers a relevant solution, copies the code into the project, makes a few edits, presses run, and the test passes.

In order to accomplish a similar task, an information-seeking agent must be able to explore a database. Similar to a human developer, it might traverse the project to gather information, such as searching for text and copying tokens from relevant documents and source code artifacts. As it traverses the project, the model constructs a project-specific knowledge graph.

%\begin{figure*}
%  \centering
%  \includegraphics[width=0.8\textwidth]{use_graph}
%  \caption{Software projects consist of many documents which share common references. These references form an entity alignment graph, with vertices decorated by the surrounding context, and edges by the relation type, e.g. exact duplicate, fuzzy match or synthetic relation (learned or engineered).}
%\end{figure*}

%During evaluation, we measure task performance across search strategies. Depending on the task in question, various loss functions are possible, from a simple string distance metric over a masked code fragment, to more complex properties (e.g. the presence or absence of errors, or some internal state of the REPL) that must be satisfied by interacting with the runtime environment. Many program analysis and repair tasks are also possible, such as defect, duplicate or vulnerability detection and correction.

\section{Method}

We fetch a dataset of repositories sampled from popular Java repositories on GitHub, containing a mixture of filetypes representing source code and natural language artifacts. From each repository, we index all substrings of every line in every file using a variable height radix tree producing a multimap of $\texttt{kwdIndex: String -> List<Location<F, O>>}$ of $\texttt{String}$  queries to file-offset pairs. We also encode CodeBERT~\cite{feng2020codebert} sequence embeddings to substrings $\texttt{knnIndex: Vector -> Location<F, O>}$ using a Hierarchial Navigavble Small World Graph~\cite{malkov2018efficient} (HNSWG).


\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{bert_embedding}
  \caption{CodeBERT takes a unicode sequence and emits a vector sequence, which we accumulate into a single vector.}
  \label{fig:bert}
\end{figure}

For each token in a string, CodeBERT emits a length-768 vector, so a line with $n$ tokens produces a matrix of shape $\mathbb R^{768 \times (n + 1)}$, the first column of which contains the final hidden layer activations. We concatenate the CodeBERT sequence `\texttt{<s>}' and using the source tokens, encode and vectorize the encoded sequence using the vocabulary, then take the first row as our sequence embedding as depicted in Fig.~\ref{fig:bert}. In \S~\ref{sec:results}, we compare various distance-metrics to fetch the nearest sequence embeddings in our database and compare precision and recall across various types of distance metrics.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{latent_kg}
  \caption{To compute our query neighborhood, we traverse the HNSWG up to max depth $d$, i.e. $d=1$ fetches the neighbors, $d=2$ fetches the neighbors-of-neighbors, and $d=3$ fetches the neighbors-of-neighbors-of-neighbors.}
  \label{fig:de2kg}
\end{figure}

For a given query and context, we first compute the context embedding and using a distance metric, fetch the k-nearest neighboring documents in latent space, forming the depth-1 nodes in the graph, and repeat this procedure recursively, pruning with a beam search heuristic based on the total path length in latent space. This procedure is depicted in Fig.~\ref{fig:de2kg}.

\begin{figure}[H]
  \centering
  \includegraphics[width=1.05\textwidth]{architecture}
  \caption{Unlike language models which directly learn the data distribution, our model is designed to query an unseen database only available at test time. The model scores and selectively expands a subset of promising results within the database using a beam search heuristic, then runs message passing over the resulting graph to obtain the final task prediction.}
  \label{fig:architecture}
\end{figure}

Once the beam search procedure is complete, we have a graph representing the neighborhood of the nearest neighboring code fragments in the parent project. This forms a so-called \textit{virtual knowledge graph}, with nodes decorated by the CodeBERT sequence embeddings and edges decorated with the direction vector between documents. We then run $p$ rounds of message passing to obtain graph embedding or \textit{neighborhood summary vector} (Fig.~\ref{fig:architecture}).

% We initialize the policy network using a pretrained language model. Starting at the site of the prediction task and conditioning on its local context, the policy network draws K queries from its latent state, which are fed into the index to produce a set of matching locations. The model scores and selectively expands a subset of those locations, and the process is repeated using finite-horizon MCTS or similar beam search procedure to retrieve a set of contextually relevant locations within the parent project.
%
%The rollout traces form a graph of related locations inside each project and their corresponding context embeddings, which together form the GNN node features. Once the rollout ends, we run message passing on the resulting GNN for a fixed numbers of steps and decode the graph embedding to obtain a task prediction. The decoder, GNN parameters, and policy network are all trained end-to-end on the downstream task, e.g. code completion, defect detection or correction. After convergence, we compare the results across horizon size and analyze the queries and filetypes which are selected.

\section{Experiments}

In this work, we attempt to understand the relationship between entities in a software project. Our research seeks to answer the following questions:

\begin{enumerate}
  \item Which contexts in a software project share mutual information?
  \item To what degree can we claim the model has learned to:\begin{enumerate}
  \item Locate contextually relevant artifacts within a software project?
  \item Comprehend the semantic content of the artifacts traversed?
  \item Apply the knowledge gathered to perform the assigned task?
  \end{enumerate}
\end{enumerate}

In contrast with classical code completion models which only require a file-local context, our method is designed to navigate an entire project. In the following experiments, we compare completion accuracy with a vanilla sequence prediction model, as well as an AST-structured sequence prediction model trained from a corpus of Java projects on the same task.

We hypothesize that by jointly learning to choose locations in a project over which to attend while solving a downstream task, such as masked sequence completion, our model will produce a feature representation capable of locating and extracting information from semantically relevant contexts. We evaluate our hypothesis both qualitatively and quantitatively.

In our first set of experiments, we try to understand what is shared between sequences mapped to similar latent vectors. Do similar sequences share salient keywords? Are those keywords relevant to the task?

In our second experiment, we try to measure the information gain from by including and excluding various filetypes through ablation. For graphs containing filetypes which include Markdown or Java, what kinds of information do these resources provide and which categories are most salient?

%In our third experiment, we compare prediction accuracy across architectures and datasets. Can we constrain the action space (e.g. only querying tokens from the surrounding context) for more efficient trajectory sampling, or allow arbitrary queries? How well does the model architecture transfer to new repositories, within and across programming languages?

In our third and final set of experiments, we compare performance across hyperparameters. Does contextual expansion lead to better task performence for a given sequence prediction task? By relaxing edge-construction criteria and increasing hyperparamers such as beam search budget, we would expect corresponding task performance to increase.

If our hypothesis is correct, the virtual knowledge graph will span both natural language and source code artifacts. If so, this would provide evidence to support the broader hypothesis~\cite{guo2017semantically} that documentation is a useful source of information. In addition to being useful for the prediction task itself, we anticipate our model could also be used for knowledge graph extraction and suggest semantically relevant code snippets to developers.


%\section{Next steps}
%
%Our next steps are to build a simple environment which allows an agent to interact with a software repository and construct a graph. We will use an in-memory filesystem to store and load project artifacts. The policy network will need to be pretrained on a corpus of projects in the same language. To model the action space, we will use the radix tree of the parent project, with transition probabilities conditioned on the local context.
%
%For further details and to get started, please visit our GitHub page: \url{https://github.com/breandan/gym-fs}.

\section{Results}\label{sec:results}

\begin{filecontents*}{data.csv}
  levdist,euclidist
  0,0.0
  1,2.3633473184373646
  2,2.4890602032343545
  3,2.911810111999512
  4,2.0028354823589325
  5,1.4084663391113281
  6,2.3637032292105933
  7,4.472905204846309
  8,2.6024305603720923
  9,2.5696596684663193
  10,2.6834484385816673
  11,3.3063877632743432
  12,4.491936294656051
  13,2.9728973135352135
  14,2.746051150560379
  15,2.7703943712669505
  16,2.823648720547773
  17,2.7505312219994966
  18,2.7979473780982103
  19,2.771831948967541
  20,2.7493412751777497
  21,3.272918408115705
  22,3.2250382444831764
  23,3.0653511084519423
  24,3.137301045987341
  25,3.46093900923459
  26,3.3380240285900276
  27,3.0999733769532405
  28,3.407973573014543
  29,3.2747946337955756
  30,3.5743897480842395
  31,3.2816565019496973
  32,3.2992073810377787
  33,3.21660378575325
  34,3.3823076827185496
  35,3.4550687623402427
  36,3.653107548516894
  37,3.9763369475092207
  38,3.9047256729559985
  39,4.075039531015287
  40,3.93511299265931
  41,4.092973797094254
  42,4.213170318250303
  43,4.095147806770948
  44,3.7521678453468414
  45,4.114031522701948
  46,4.2242398541252895
  47,3.876007026639478
  48,3.6982129307116494
  49,3.817896408912463
  50,3.8123655733854873
  51,3.825625277402108
  52,3.653673952701045
  53,3.747180778153089
  54,3.5404654390671673
  55,3.8010367041542414
  56,3.859985693081005
  57,4.078702474044541
  58,4.01475347223736
  59,3.8205209225416183
  60,3.777980612773521
  61,3.84536190032959
  62,3.908125676970551
  63,4.051204487255641
  64,4.066887232992384
  65,4.028644317930395
  66,4.119296615406618
  67,4.142942777494105
  68,4.540319035130162
  69,4.221329057538831
  70,4.1877835669168615
  71,4.422936993975972
  72,5.149443597629152
  73,5.065425028119768
  74,4.604162693023682
  75,5.423289246029324
  76,5.14295246050908
  77,5.388673737645149
  78,5.435530768500434
  79,6.382849454879761
  80,4.942715644836426
  81,5.410255829493205
  82,6.678394447673451
  83,6.862033176422119
  84,6.754118633270264
  86,4.69812536239624
  87,4.876834869384766
  88,4.781426429748535
  89,4.878495693206787
  90,4.8594865798950195
  91,5.583544492721558
  92,5.392730985369001
  93,5.439589553409153
  94,5.195530796051026
  95,5.476902825491769
  96,5.9483455419540405
  97,5.545152568817139
  98,6.094607421330044
  99,5.674917125701905
  100,6.297649145126343
  101,6.044215083122253
  102,6.99380362033844
  103,5.960549831390381
  106,7.0842125415802
  107,7.431237485673693
\end{filecontents*}

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[width]
    \begin{axis}[title={String Edit vs. Euclidean distance}, width=\textwidth, height=0.4\textwidth]
      \addplot table [x=levdist, y=euclidist, col sep=comma] {data.csv};
    \end{axis}
  \end{tikzpicture}
  \caption{Levenstein edit distance and average Euclidean distance in CodeBERT latent space appears to be positively correlated.}
  \end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.55\textwidth]{embeddings}
  \caption{Reduced dimensionality TNSE embeddings colored by line length.}
\end{figure}

  \bibliography{research_proposal}
  \bibliographystyle{plain}
\end{document}